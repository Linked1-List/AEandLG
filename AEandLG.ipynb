{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linked1-List/AEandLG/blob/main/AEandLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM24AUdlEKFQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvgMemU_5Fcf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATeQ15JBEMxy"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "\n",
        "# 음성 파일을 로드하는 함수\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
        "    return audio, sr\n",
        "\n",
        "# 음성을 청크로 나누는 함수\n",
        "def split_audio(audio, sr, chunk_length_sec=5, output_dir='output_audio'):\n",
        "    # chunk 길이를 샘플로 변환 (초 -> 샘플)\n",
        "    chunk_length_samples = chunk_length_sec * sr\n",
        "\n",
        "    # 저장할 디렉토리 생성 (없으면)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 청크로 나누어서 저장\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_samples):\n",
        "        chunk = audio[i:i+chunk_length_samples]\n",
        "        chunk_filename = f'chunk_{i // chunk_length_samples + 1}.wav'\n",
        "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
        "        sf.write(chunk_path, chunk, sr)  # chunk 저장\n",
        "        chunks.append(chunk_path)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# 메인 실행 부분\n",
        "def main(input_audio_path, output_dir='output_audio', chunk_length_sec=5):\n",
        "    # 오디오 파일 로드\n",
        "    audio, sr = load_audio(input_audio_path)\n",
        "\n",
        "    # 오디오를 청크로 나누고 저장\n",
        "    chunks = split_audio(audio, sr, chunk_length_sec, output_dir)\n",
        "\n",
        "    # 저장된 청크 파일 리스트 출력\n",
        "    print(f'총 {len(chunks)}개의 청크가 {output_dir} 디렉토리에 저장되었습니다.')\n",
        "    return chunks\n",
        "\n",
        "# 사용 예시\n",
        "input_audio_path = '/content/drive/MyDrive/data/chosubin.mp3'  # 원본 음성 파일 경로\n",
        "output_dir = '/content/drive/MyDrive/humanchunk'     # 분할된 파일을 저장할 디렉토리\n",
        "chunks = main(input_audio_path, output_dir, chunk_length_sec=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJRu9LxZKluq"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "\n",
        "# 음성 파일을 로드하는 함수\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
        "    return audio, sr\n",
        "\n",
        "# 음성을 청크로 나누는 함수\n",
        "def split_audio(audio, sr, chunk_length_sec=5, output_dir='output_audio'):\n",
        "    # chunk 길이를 샘플로 변환 (초 -> 샘플)\n",
        "    chunk_length_samples = chunk_length_sec * sr\n",
        "\n",
        "    # 저장할 디렉토리 생성 (없으면)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 청크로 나누어서 저장\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_samples):\n",
        "        chunk = audio[i:i+chunk_length_samples]\n",
        "        chunk_filename = f'chunk_{i // chunk_length_samples + 1}.wav'\n",
        "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
        "        sf.write(chunk_path, chunk, sr)  # chunk 저장\n",
        "        chunks.append(chunk_path)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# 메인 실행 부분\n",
        "def main(input_audio_path, output_dir='output_audio', chunk_length_sec=5):\n",
        "    # 오디오 파일 로드\n",
        "    audio, sr = load_audio(input_audio_path)\n",
        "\n",
        "    # 오디오를 청크로 나누고 저장\n",
        "    chunks = split_audio(audio, sr, chunk_length_sec, output_dir)\n",
        "\n",
        "    # 저장된 청크 파일 리스트 출력\n",
        "    print(f'총 {len(chunks)}개의 청크가 {output_dir} 디렉토리에 저장되었습니다.')\n",
        "    return chunks\n",
        "\n",
        "# 사용 예시\n",
        "input_audio_path = '/content/drive/MyDrive/data/새+프로젝트.mp3'  # 원본 음성 파일 경로\n",
        "output_dir = '/content/drive/MyDrive/aichunk'     # 분할된 파일을 저장할 디렉토리\n",
        "chunks = main(input_audio_path, output_dir, chunk_length_sec=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sBnV-f6--qm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# 오토인코더 모델 정의\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# 음성 파일을 Mel-spectrogram으로 변환하는 함수\n",
        "def audio_to_mel_spectrogram(waveform, sample_rate=16000, target_length=16000):\n",
        "    # Mel-spectrogram 변환\n",
        "    mel_spec = T.MelSpectrogram(sample_rate=sample_rate)(waveform)  # Mel-Spectrogram 생성\n",
        "    mel_spec_db = T.AmplitudeToDB()(mel_spec)  # 로그 스펙트로그램으로 변환\n",
        "\n",
        "    # 패딩 또는 자르기 (길이를 target_length로 맞추기)\n",
        "    num_samples = mel_spec_db.shape[-1]\n",
        "\n",
        "    if num_samples < target_length:\n",
        "        # 패딩: 짧은 음성 데이터를 0으로 채워서 길이를 맞춤\n",
        "        padding = target_length - num_samples\n",
        "        mel_spec_db = torch.cat([mel_spec_db, torch.zeros(1, mel_spec_db.shape[1], padding)], dim=-1)\n",
        "    elif num_samples > target_length:\n",
        "        # 자르기: 긴 음성 데이터를 target_length로 잘라냄\n",
        "        mel_spec_db = mel_spec_db[:, :, :target_length]\n",
        "\n",
        "    return mel_spec_db\n",
        "\n",
        "# 음성 파일 로드 및 전처리 함수\n",
        "def load_audio_file(file_path, sample_rate=16000, target_length=16000):\n",
        "    waveform, original_sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # 샘플링 레이트가 다르면 변환\n",
        "    if original_sample_rate != sample_rate:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=sample_rate)(waveform)\n",
        "\n",
        "    # 정규화: 음성 데이터를 [-1, 1] 범위로 정규화\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    # Mel-spectrogram으로 변환 및 길이 맞추기\n",
        "    mel_spectrogram = audio_to_mel_spectrogram(waveform, sample_rate, target_length)\n",
        "\n",
        "    return mel_spectrogram\n",
        "\n",
        "# 데이터셋 로드 함수\n",
        "def load_dataset(file_paths, sample_rate=16000, target_length=16000):\n",
        "    spectrograms = []\n",
        "    for file_path in file_paths:\n",
        "        mel_spectrogram = load_audio_file(file_path, sample_rate, target_length)\n",
        "        spectrograms.append(mel_spectrogram)\n",
        "    return torch.stack(spectrograms)  # [N, 1, H, W] 형식으로 반환\n",
        "\n",
        "# 데이터 로딩\n",
        "human_data_dir = '/content/drive/MyDrive/humanchunk'\n",
        "human_files = [os.path.join(human_data_dir, f) for f in os.listdir(human_data_dir) if f.endswith('.wav')]\n",
        "human_spectrograms = load_dataset(human_files)\n",
        "\n",
        "ai_data_dir = '/content/drive/MyDrive/aichunk'\n",
        "ai_files = [os.path.join(ai_data_dir, f) for f in os.listdir(ai_data_dir) if f.endswith('.wav')]\n",
        "ai_spectrograms = load_dataset(ai_files)\n",
        "\n",
        "# DataLoader 설정\n",
        "batch_size = 8\n",
        "human_dataset = TensorDataset(human_spectrograms)\n",
        "dataloader = DataLoader(human_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 모델 및 손실 함수, 옵티마이저 설정\n",
        "model = ConvAutoencoder()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 모델 학습\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    for batch in dataloader:\n",
        "        input_spectrograms = batch[0]\n",
        "\n",
        "        # 모델 예측 및 손실 계산\n",
        "        decoded = model(input_spectrograms)\n",
        "        loss = criterion(decoded, input_spectrograms)\n",
        "\n",
        "        # 역전파 및 최적화\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {loss.item():.4f}')\n",
        "\n",
        "# 재구성 오차 계산 함수\n",
        "def calculate_reconstruction_error(spectrogram):\n",
        "    with torch.no_grad():\n",
        "        decoded = model(spectrogram.unsqueeze(0))  # [1, 1, H, W] 형식으로 입력\n",
        "        error = criterion(decoded, spectrogram.unsqueeze(0)).item()\n",
        "    return error\n",
        "\n",
        "# 인간 음성의 개별 재구성 오차 출력\n",
        "print(\"\\nHuman Voice Reconstruction Errors:\")\n",
        "humansum=0\n",
        "cnth=0\n",
        "for i, human_spectrogram in enumerate(human_spectrograms):\n",
        "    error = calculate_reconstruction_error(human_spectrogram)\n",
        "    print(f\"Human Sample {i+1}: Reconstruction Error = {error:.4f}\")\n",
        "    if error>=0:\n",
        "      humansum+=error\n",
        "      cnth=i+1\n",
        "print(humansum/cnth)\n",
        "\n",
        "# AI 음성의 개별 재구성 오차 출력\n",
        "print(\"\\nAI Voice Reconstruction Errors:\")\n",
        "aisum=0\n",
        "cnta=0\n",
        "for i, ai_spectrogram in enumerate(ai_spectrograms):\n",
        "    error = calculate_reconstruction_error(ai_spectrogram)\n",
        "    print(f\"AI Sample {i+1}: Reconstruction Error = {error:.4f}\")\n",
        "    if error>=0:\n",
        "      aisum+=error\n",
        "      cnta=i+1\n",
        "print(aisum/cnta)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 완료 후\n",
        "model_save_path = \"/content/drive/MyDrive/autoencoder/autoencoder_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "gBHi9nTxWm2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import os\n",
        "import soundfile as sf\n",
        "\n",
        "# 음성 파일을 로드하는 함수\n",
        "def load_audio(file_path, target_sr=16000):\n",
        "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
        "    return audio, sr\n",
        "\n",
        "# 음성을 청크로 나누는 함수\n",
        "def split_audio(audio, sr, chunk_length_sec=5, output_dir='output_audio'):\n",
        "    # chunk 길이를 샘플로 변환 (초 -> 샘플)\n",
        "    chunk_length_samples = chunk_length_sec * sr\n",
        "\n",
        "    # 저장할 디렉토리 생성 (없으면)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # 청크로 나누어서 저장\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_samples):\n",
        "        chunk = audio[i:i+chunk_length_samples]\n",
        "        chunk_filename = f'chunk_{i // chunk_length_samples + 1}.wav'\n",
        "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
        "        sf.write(chunk_path, chunk, sr)  # chunk 저장\n",
        "        chunks.append(chunk_path)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# 메인 실행 부분\n",
        "def main(input_audio_path, output_dir='output_audio', chunk_length_sec=5):\n",
        "    # 오디오 파일 로드\n",
        "    audio, sr = load_audio(input_audio_path)\n",
        "\n",
        "    # 오디오를 청크로 나누고 저장\n",
        "    chunks = split_audio(audio, sr, chunk_length_sec, output_dir)\n",
        "\n",
        "    # 저장된 청크 파일 리스트 출력\n",
        "    print(f'총 {len(chunks)}개의 청크가 {output_dir} 디렉토리에 저장되었습니다.')\n",
        "    return chunks\n",
        "\n",
        "# 사용 예시\n",
        "input_audio_path = '/content/drive/MyDrive/data/새+프로젝트 (2).mp3'  # 원본 음성 파일 경로\n",
        "output_dir = '/content/drive/MyDrive/data/ai'     # 분할된 파일을 저장할 디렉토리\n",
        "chunks = main(input_audio_path, output_dir, chunk_length_sec=2)\n"
      ],
      "metadata": {
        "id": "ASezs47FS-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# 오토인코더 모델 정의\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# 음성 파일을 Mel-spectrogram으로 변환하는 함수\n",
        "def audio_to_mel_spectrogram(waveform, sample_rate=16000, target_length=16000):\n",
        "    # Mel-spectrogram 변환\n",
        "    mel_spec = T.MelSpectrogram(sample_rate=sample_rate)(waveform)  # Mel-Spectrogram 생성\n",
        "    mel_spec_db = T.AmplitudeToDB()(mel_spec)  # 로그 스펙트로그램으로 변환\n",
        "\n",
        "    # 패딩 또는 자르기 (길이를 target_length로 맞추기)\n",
        "    num_samples = mel_spec_db.shape[-1]\n",
        "\n",
        "    if num_samples < target_length:\n",
        "        # 패딩: 짧은 음성 데이터를 0으로 채워서 길이를 맞춤\n",
        "        padding = target_length - num_samples\n",
        "        mel_spec_db = torch.cat([mel_spec_db, torch.zeros(1, mel_spec_db.shape[1], padding)], dim=-1)\n",
        "    elif num_samples > target_length:\n",
        "        # 자르기: 긴 음성 데이터를 target_length로 잘라냄\n",
        "        mel_spec_db = mel_spec_db[:, :, :target_length]\n",
        "\n",
        "    return mel_spec_db\n",
        "\n",
        "# 음성 파일 로드 및 전처리 함수\n",
        "def load_audio_file(file_path, sample_rate=16000, target_length=16000):\n",
        "    waveform, original_sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # 샘플링 레이트가 다르면 변환\n",
        "    if original_sample_rate != sample_rate:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=sample_rate)(waveform)\n",
        "\n",
        "    # 정규화: 음성 데이터를 [-1, 1] 범위로 정규화\n",
        "    waveform = waveform / waveform.abs().max()\n",
        "\n",
        "    # Mel-spectrogram으로 변환 및 길이 맞추기\n",
        "    mel_spectrogram = audio_to_mel_spectrogram(waveform, sample_rate, target_length)\n",
        "\n",
        "    return mel_spectrogram\n",
        "\n",
        "# 데이터셋 로드 함수\n",
        "def load_dataset(file_paths, sample_rate=16000, target_length=16000):\n",
        "    spectrograms = []\n",
        "    for file_path in file_paths:\n",
        "        mel_spectrogram = load_audio_file(file_path, sample_rate, target_length)\n",
        "        spectrograms.append(mel_spectrogram)\n",
        "    return torch.stack(spectrograms)  # [N, 1, H, W] 형식으로 반환\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 재구성 오차 계산 함수\n",
        "def calculate_reconstruction_error(spectrogram):\n",
        "    with torch.no_grad():\n",
        "        decoded = model(spectrogram.unsqueeze(0))  # [1, 1, H, W] 형식으로 입력\n",
        "        error = criterion(decoded, spectrogram.unsqueeze(0)).item()\n",
        "    return error\n",
        "\n",
        "# 모델 로드\n",
        "model_save_path = \"/content/drive/MyDrive/autoencoder/autoencoder_model.pth\"\n",
        "model = ConvAutoencoder()\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# 데이터 로딩\n",
        "new_data_dir = '/content/drive/MyDrive/data/ai'\n",
        "new_files = [os.path.join(new_data_dir, f) for f in os.listdir(new_data_dir) if f.endswith('.wav')]\n",
        "new_spectrograms = load_dataset(new_files)\n",
        "\n",
        "# 새 음성의 개별 재구성 오차 출력\n",
        "print(\"\\nNew Voice Reconstruction Errors:\")\n",
        "nsum=0\n",
        "cntn=0\n",
        "for i, new_spectrogram in enumerate(new_spectrograms):\n",
        "    error = calculate_reconstruction_error(new_spectrogram)\n",
        "    print(f\"New Sample {i+1}: Reconstruction Error = {error:.4f}\")\n",
        "    if error>=0:\n",
        "      nsum+=error\n",
        "      cntn=i+1\n",
        "print(nsum/cntn)"
      ],
      "metadata": {
        "id": "ivlJfAA-WvXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. CSV 파일 불러오기\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/data/reconstruction_errors.csv\")  # 같은 디렉토리에 있는 파일 사용\n",
        "\n",
        "# 2. 데이터 전처리\n",
        "errors = df[\"error\"].values\n",
        "labels = df[\"label\"].values\n",
        "\n",
        "# 3. 스케일링\n",
        "scaler = StandardScaler()\n",
        "errors_scaled = scaler.fit_transform(errors.reshape(-1, 1))\n",
        "\n",
        "# 4. 학습/테스트 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(errors_scaled, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. 로지스틱 회귀 모델 학습\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. 예측 및 평가\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 7. 평가 결과 출력\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# 8. 특정 Reconstruction Error에 대한 예측 확률 함수\n",
        "def predict_probabilities(reconstruction_error):\n",
        "    scaled_error = scaler.transform(np.array([[reconstruction_error]]))\n",
        "    probabilities = model.predict_proba(scaled_error)\n",
        "    human_prob, ai_prob = probabilities[0]\n",
        "    return human_prob, ai_prob\n",
        "\n",
        "# 9. 예시 사용\n",
        "test_error = 53.9091\n",
        "human_prob, ai_prob = predict_probabilities(test_error)\n",
        "\n",
        "print(f\"\\nReconstruction Error = {test_error}\")\n",
        "print(f\"Probability of Human: {human_prob:.2%}\")\n",
        "print(f\"Probability of AI: {ai_prob:.2%}\")\n"
      ],
      "metadata": {
        "id": "ciJ7WDDYVZ5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}